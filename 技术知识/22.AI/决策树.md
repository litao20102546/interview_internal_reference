1. 基本概念
	1. 信息量
       * 意外越大，越不可能发生，概率就越小，信息量也就越大
       * 信息量= log2(1/概率)=log2(概率^-1)=-log2(概率)
	2. 熵
       * 熵=H=-sum(概率*log2(概率))
       * 分类越多->信息量越大->熵越大
       * 分类越平均->熵越大
	3. 信息增益
       * 信息增益(Information Gain)：熵A-条件熵B，是信息量的差值
       * 好的条件就是信息增益越大越好，即变化完后熵越小越好（熵代表混乱程度，最大程度地减小了混乱）
	4. 信息增益率
       * 信息增益/自身熵值
	5. 评价函数
       * sum(Nt(叶子节点)* H(t)) , Nt是叶子节点的实例个数

2. 决策树剪枝
   * 预剪枝——指在构造决策树的同时进行剪枝。在创建分支的过程中，为了避免过拟合，可以设定一个阈值，熵减少小于阈值时，则停止继续创建分支。实际效果并不好
   * 后剪枝——指在决策树构建完成之后回溯，进行剪枝。对拥有相同父节点的一组节点进行检查，判断如果将其合并，熵增加量是否小于某一阈值？是，则合并为一个节点，其中包含了所有可能的结果。后剪枝是删除一些子树，然后用叶子节点代替。此叶子节点的类别通过多数原则确定——用子树中大多数训练样本所属的类别来标识。算法包括Reduced-Error Pruning、Pessimistic Error Pruning(悲观剪枝)
   * 预剪枝可能产生欠拟合的风险，后剪枝由于需要先生成完整的树，再自底向上进行剪枝，因此花费的时间要久的多。