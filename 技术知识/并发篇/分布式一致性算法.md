https://blog.csdn.net/zwj12041743/article/details/99679652
https://www.jianshu.com/p/ee645aeba365

# 评价指标

一致性的目标是使一组服务器在一个值上达成一致，所以活跃的特征在于最终每个服务器都可以决定一个值。安全性表明没有两台服务器来设定值。

* 一致性

* 复制
* 领导人选举
* 安全
* 活跃度
* 



# ZAB

ZAB协议是Multi-Paxos协议的变形，也是只有一个Leader能够接收写操作请求。

ZAB协议中主要有两种模式，第一是消息广播模式；第二是崩溃恢复模式。

## 消息广播模式

1. 客户端发起一个写操作请求。
2. Leader服务器将客户端的request请求转化为事务proposql提案，同时为每个proposal分配一个全局唯一的ID，即ZXID。
3. leader服务器与每个follower之间都有一个队列，leader将消息发送到该队列。
4. follower机器从队列中取出消息处理(写入本地事务日志中)完毕后，向leader服务器发送ACK确认。
5. leader服务器收到半数以上的follower的ACK后，即认为可以发送commit。
6. leader向所有的follower服务器发送commit消息。

## 崩溃恢复模式

1. zookeeper集群中为保证任何所有进程能够有序的顺序执行，只能是leader服务器接受写请求，即使是follower服务器接受到客户端的请求，也会转发到leader服务器进行处理。
2. 如果leader服务器发生崩溃，则zab协议要求zookeeper集群进行崩溃恢复和leader服务器选举。
3. ZAB协议崩溃恢复要求满足如下2个要求：
   * 确保已经被leader提交的proposal必须最终被所有的follower服务器提交。
   * 确保丢弃已经被leader出的但是没有被提交的proposal。

4. 根据上述要求，新选举出来的leader不能包含未提交的proposal，即新选举的leader必须都是已经提交了的proposal的follower服务器节点。同时，新选举的leader节点中含有最高的ZXID。这样做的好处就是可以避免了leader服务器检查proposal的提交和丢弃工作。
5. leader服务器发生崩溃时分为如下场景：
   * leader在提出proposal时未提交之前崩溃，则经过崩溃恢复之后，新选举的leader一定不能是刚才的leader。因为这个leader存在未提交的proposal。
   * leader在发送commit消息之后，崩溃。即消息已经发送到队列中。经过崩溃恢复之后，参与选举的follower服务器(刚才崩溃的leader有可能已经恢复运行，也属于follower节点范畴)中有的节点已经是消费了队列中所有的commit消息。即该follower节点将会被选举为最新的leader。剩下动作就是数据同步过程。

## 选举的机制关键点

* 外部投票 内部投票 选举轮次 PK
* vote_sid vote_zxid self_sid self_zxid
* 通常那台服务器上的数据越新（ZXID会越大），其成为Leader的可能性越大，也就越能够保证数据的恢复

1. 服务器启动时期的Leader选举
   1. 每个Server发出一个投票(SID, ZXID)
   2. 接受来自各个服务器的投票 判断投票有效性(是否是本轮投票、是否来自LOOKING状态)
   3. 处理投票
      * 优先检查ZXID, ZXID比较大的服务器优先作为Leader
      * 如果ZXID相同，那么就比较myid, myid较大的服务器作为Leader服务器
   4. 统计投票 判断是否已经有过半机器接受到相同的投票信息
   5. 改变服务器状态 Follower-> FOLLOWING，Leader -> LEADING

2. 服务器运行时的leader选举
   运行时即便当有非Leader服务器宕机或新加入，此时也不会影响Leader, 但是当leader崩溃或者leader失去大多数的follower，这时zk进入恢复模式，恢复模式需要重新选举出一个新的leader，让所有的Server都恢复到一个正确的状态。
     1. 变更状态 LOOKING
     2. 每个Server会发出一个投票(SID, ZXID)
     3. 接收来自各个服务器的投票
     4. 处理投票
     5. 统计投票
     6. 改变服务器的状态


<img src="/Users/orangeli/litao/FindWork/interview_internal_reference/技术知识/12.Zookeeper篇/zk_zab_basic_paxos.png" />

2、Zookeeper选主流程(fast paxos)
fast paxos流程是在选举过程中，某Server首先向所有Server提议自己要成为leader，当其它Server收到提议以后，解决epoch和 zxid的冲突，并接受对方的提议，然后向对方发送接受提议完成的消息，重复这个流程，最后一定能选举出Leader。

<img src="/Users/orangeli/litao/FindWork/interview_internal_reference/技术知识/12.Zookeeper篇/zk_zab_fast_paxos.png" />

3. QuorumCnxManager
   每台服务器在启动的过程中，会启动一个QuorumPeerManager，负责各台服务器之间的底层Leader选举过程中的网络通信。
   1. 消息队列 recvQueue queueSendMap senderWorkerMap lastMessageSent
   2. 建立连接 创建一个ServerSocket来监听Leader选举的通信端口
4. enhancement
   * Zk的选举算法有两种:basic paxos + fast paxos, 3.4.0后的zookeeper的版本只保留了FastLeader
   * 为了保证事务的顺序一致性，zookeeper采用了递增的事务id号（zxid）来标识事务。所有的提议（proposal）都在被提出的时候加上了zxid。实现中zxid是一个64位的数字，它高32位是epoch用来标识leader关系是否改变，每次一个leader被选出来，它都会有一个新的epoch，标识当前属于那个leader的统治时期。低32位用于递增计数。
   * 每个Server在工作过程中有三种状态：
     * LOOKING：当前Server不知道leader是谁，正在搜寻
     * LEADING：当前Server即为选举出来的leader
     * FOLLOWING：leader已经选举出来，当前Server与之同步
   * Zookeeper的核心是原子广播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议。Zab协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数Server完成了和leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和Server具有相同的系统状态。
   * 保证的特点
     * 更新请求顺序进行，来自同一个client的更新请求按其发送顺序依次执行
     * 数据更新原子性，一次数据更新要么成功，要么失败
     * 全局唯一数据视图，client无论连接到哪个server，数据视图都是一致的
     * 实时性，在一定事件范围内，client能读到最新数据

# Raft

## 状态

raft协议中，一个节点任一时刻处于以下三个状态之一：

* leader

* follower

* candidate
  给出状态转移图能很直观的直到这三个状态的区别

  <img src=raft状态.png />


## 选举过程详解

上面已经说过，如果follower在election timeout内没有收到来自leader的心跳，（也许此时还没有选出leader，大家都在等；也许leader挂了；也许只是leader与该follower之间网络故障），则会主动发起选举。步骤如下：

1. 增加节点本地的 current term ，切换到candidate状态
2. 投自己一票
3. 并行给其他节点发送 RequestVote RPCs
4. 等待其他节点的回复
   在这个过程中，根据来自其他节点的消息，可能出现三种结果：
   * 收到majority的投票（含自己的一票），则赢得选举，成为leader
   * 被告知别人已当选，那么自行切换到follower
   * 一段时间内没有收到majority投票，则保持candidate状态，重新发出选举

第一种情况，赢得了选举之后，新的leader会立刻给所有节点发消息，广而告之，避免其余节点触发新的选举。在这里，先回到投票者的视角，投票者如何决定是否给一个选举请求投票呢，有以下约束：

* 在任一任期内，单个节点最多只能投一票
* 候选人知道的信息不能比自己的少（AppendEntries，先比较logTerm，再比较logIndex）
* first-come-first-served 先来先得

第二种情况，比如有三个节点A B C。A B同时发起选举，而A的选举消息先到达C，C给A投了一票，当B的消息到达C时，已经不能满足上面提到的第一个约束，即C不会给B投票，而A和B显然都不会给对方投票。A胜出之后，会给B,C发心跳消息，节点B发现节点A的term不低于自己的term，知道有已经有Leader了，于是转换成follower。



第三种情况，没有任何节点获得majority投票，出现了平票 split vote的情况。这个时候大家都在等啊等，直到超时后重新发起选举。如果出现平票的情况，那么就延长了系统不可用的时间（没有leader是不能处理客户端写请求的），因此raft引入了randomized election timeouts来尽量避免平票情况。同时，leader-based 共识算法中，节点的数目都是奇数个，尽量保证majority的出现。

## log replication请求完整流程

当系统（leader）收到一个来自客户端的写请求，到返回给客户端，整个过程从leader的视角来看会经历以下步骤：

leader append log entry
leader issue AppendEntries RPC in parallel
leader wait for majority response
leader apply entry to state machine
leader reply to client
leader notify follower apply log
这里有两个词：commit（committed），apply(applied)，前者是指日志被复制到了大多数节点后日志的状态；而后者则是节点将日志应用到状态机，真正影响到节点状态。

<img src=log-replication.png style="zoom:50%;" />

## log matching

log匹配特性， 就是说如果两个节点上的某个log entry的log index相同且term相同，那么在该index之前的所有log entry应该都是相同的。

* leader会维护一个nextIndex[]数组，记录了leader可以发送每一个follower的log index，初始化为eader最后一个log index加1， 前面也提到，leader选举成功之后会立即给所有follower发送AppendEntries RPC（不包含任何log entry， 也充当心跳消息）,那么流程总结为：

leader 初始化nextIndex[x]为 leader最后一个log index + 1
AppendEntries里prevLogTerm prevLogIndex来自 logs[nextIndex[x] - 1]
如果follower判断prevLogIndex位置的log term不等于prevLogTerm，那么返回 false，否则返回True
leader收到follower的恢复，如果返回值是True，则nextIndex[x] -= 1, 跳转到2。 否则
同步nextIndex[x]后的所有log entries

leader完整性：如果一个log entry在某个任期被提交（committed），那么这条日志一定会出现在所有更高term的leader的日志里面。这个跟leader election、log replication都有关。

一个日志被复制到majority节点才算committed
一个节点得到majority的投票才能成为leader，而节点A给节点B投票的其中一个前提是，B的日志不能比A的日志旧。下面的引文指处了如何判断日志的新旧
上面两点都提到了majority：commit majority and vote majority，根据Quorum，这两个majority一定是有重合的，因此被选举出的leader一定包含了最新的committed的日志。

raft与其他协议（Viewstamped Replication、mongodb）不同，raft始终保证leade包含最新的已提交的日志，因此leader不会从follower catchup日志，这也大大简化了系统的复杂度。

为了防止前任日志提交后被覆盖，某个leader选举成功之后，不会直接提交前任leader时期的日志，而是通过提交当前任期的日志的时候“顺手”把之前的日志也提交了，在任期开始的时候发立即尝试复制、提交一条空的log

## Basic Paxos

通过一个决议分为两个阶段：

1. prepare阶段：

* proposer选择一个提案编号n并将prepare请求发送给acceptors中的一个多数派；
* acceptor收到prepare消息后，如果提案的编号大于它已经回复的所有prepare消息(回复消息表示接受accept)，则acceptor将自己上次接受的提案回复给proposer，并承诺不再回复小于n的提案；

2. 批准阶段：

* 当一个proposer收到了多数acceptors对prepare的回复后，就进入批准阶段。它要向回复prepare请求的acceptors发送accept请求，包括编号n和根据P2c决定的value（如果根据P2c没有已经接受的value，那么它可以自由决定value）。
* 在不违背自己向其他proposer的承诺的前提下，acceptor收到accept请求后即批准这个请求。

## Multi-Paxos

Paxos的典型部署需要一组连续的被接受的值（value），作为应用到一个分布式状态机的一组命令。如果每个命令都通过一个Basic Paxos算法实例来达到一致，会产生大量开销。

如果Leader是相对稳定不变的，第1阶段就变得不必要。 这样，系统可以在接下来的Paxos算法实例中，跳过的第1阶段，直接使用同样的Leader。

为了实现这一目的，在同一个Leader执行每轮Paxos算法时，提案编号 I 每次递增一个值，并与每个值一起发送。Multi-Paxos在没有故障发生时，将消息延迟(从propose阶段到learn阶段)从4次延迟降低为2次延迟。





参考网址：





