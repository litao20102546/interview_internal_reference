# 死锁

1. 死锁
   死锁是由于两个或以上的线程互相持有对方需要的资源，导致这些线程处于等待状态，无法执行。

   1. 产生死锁的四个必要条件
      * 互斥性：线程对资源的占有是排他性的，一个资源只能被一个线程占有，直到释放。
      * 请求和保持条件：一个线程对请求被占有资源发生阻塞时，对已经获得的资源不释放。
      * 不剥夺：一个线程在释放资源之前，其他的线程无法剥夺占用。
      * 循环等待：发生死锁时，线程进入死循环，永久阻塞。
   2. 产生死锁的原因
      * 竞争不可抢占性资源
         * p1已经打开F1，想去打开F2，p2已经打开F2，想去打开F1，但是F1和F2都是不可抢占的，这是发生死锁。
      * 竞争可消耗资源引起死锁
         * 进程间通信，如果顺序不当，会产生死锁，比如p1发消息m1给p2，p1接收p3的消息m3，p2接收p1的m1，发m2给p3，p3，以此类推，如果进程之间是先发信息的那么可以完成通信，但如果是先接收信息就会产生死锁。
      * 进程推进顺序不当
         * 进程在运行过程中，请求和释放资源的顺序不当，也同样会导致产生进程死锁。
   3. 避免死锁的方法
      * 破坏“互斥使用/资源独占”条件
         * 使用资源转换技术，将独占资源变为共享资源
      * 破坏“请求和保持”条件
         * 方案1：每个进程在运行前必须一次性的申请它所要求的全部资源，且仅当该进程所要的资源均可满足时才一次性的分配。
         * 资源利用率低，“饥饿”现象
         * 方案2：在允许进程动态申请资源的前提下规定，一个进行在申请新资源，且不能立即得到满足，必须释放已占有的全部资源。若需要再重新申请
      * 破坏“不可抢占”条件
         * 可以通过操作系统抢占这一资源（根据进程的不同优先级）
         * 局限性：适用于状态易于保存和恢复的资源，如CPU(抢占式的调度算法)，内存(页面置换算法)
      * 破坏“循环等待”条件
         * 通过定义资源类型的线性顺序实现
         * 方案：资源有序分配法。（如哲学家就餐问题）也就是把资源中所有的资源编号，进程在申请资源时，必须严格按照资源编号的递增次序进行，否则操作系统不予分配。（资源使用的频繁性？）
      * 银行家算法 request < available && request < needing;
         * 如果系统现存的资源可以满足它的最大需求量则按当前的申请量分配资源，否则就推迟分配
         * 银行家算法在避免死锁角度上非常有效，但是需要在进程运行前就知道其所需资源的最大值

   
# 活锁

* 活锁：是指线程1可以使用资源，但它很礼貌，让其他线程先使用资源，线程2也可以使用资源，但它很绅士，也让其他线程先使用资源。这样你让我，我让你，最后两个线程都无法使用资源。

* 解决方法：
  * 先来的先执行

当多个事务请求锁定同一个数据库对象时，系统应该按请求锁定的先后次序对这些事务进行排队。一旦释放数据库对象上的锁，就批准请求队列中的下一个事务的请求，使其锁定数据库对象，以便完成数据库操作，及时结束事务。尽可能预防&避免（嘴上说说）

# 饥饿

* 饥饿：是指如果线程T1占用了资源R，线程T2又请求封锁R，于是T2等待。T3也请求资源R，当T1释放了R上的封锁后，系统首先批准了T3的请求，T2仍然等待。然后T4又请求封锁R，当T3释放了R上的封锁之后，系统又批准了T4的请求…，T2可能永远等待。
* 高优先级的线程占用了大部分的cpu时间，低优先级线程发生饥饿
* 解决办法：
  * 解决饥饿现象的方法就是实现公平，保证所有线程都公平的获得执行的机会。

# 优先级反转

优先级反转是指一个低优先级的任务持有一个被高优先级任务所需要的共享资源。高优先任务由于因资源缺乏而处于受阻状态，一直等到低优先级任务释放资源为止。而低优先级获得的CPU时间少，如果此时有优先级处于两者之间的任务，并且不需要那个共享资源，则该中优先级的任务反而超过这两个任务而获得CPU时间。如果高优先级等待资源时不是阻塞等待，而是忙循环，则可能永远无法获得资源，因为此时低优先级进程无法与高优先级进程争夺CPU时间，从而无法执行，进而无法释放资源，造成的后果就是高优先级任务无法获得资源而继续推进。

解决方案：

* 设置优先级上限，给临界区一个高优先级，进入临界区的进程都将获得这个高优先级，如果其他试图进入临界区的进程的优先级都低于这个高优先级，那么优先级反转就不会发生。
* 优先级继承，当一个高优先级进程等待一个低优先级进程持有的资源时，低优先级进程将暂时获得高优先级进程的优先级别，在释放共享资源后，低优先级进程回到原来的优先级别。嵌入式系统VxWorks就是采用这种策略。

# 护航现象（Lock Convoys）

Lock Convoys是在多线程并发环境下由于锁的使用而引起的性能退化问题。

当多个相同优先级的线程频繁地争抢同一个锁时可能会引起lockconvoys问题，一般而言，lockconvoys并不会像deadlock或livelock那样造成应用逻辑停止不前，相反地，遭受lock convoys的系统或应用程序仍然往前运行，但是，由于线程们频繁地争抢锁而导致过多的线程环境切换，从而使得系统的运行效率大为降低，而且，若存在同等优先级下不参与锁争抢的线程，则它们可以获得相对较多的处理器资源，从而造成系统调度的不公平性。

本文将解释lockconvoys问题的缘由。

假设一组线程在频繁地获取锁（所谓频繁，指在一个时间片的执行周期内多次获取锁），比如在Windows应用程序中常常用临界区（criticalsection）来保护一个共享变量或者防止一段代码被重入，这是极有可能发生的。

假设线程A获取到了锁，这时发生了线程调度中断，它的时间片用完了，于是，系统调度器交给下一个线程执行，不妨设线程B获得了执行权。由于此锁被线程A获取，所以，当线程B执行到获取锁的操作时，虽然时间片未用完，但不得不放弃执行权。如此继续，所有同等优先级且要竞争此锁的线程都被阻塞。调度器再次回到线程A，很快地线程A释放了锁。在操作系统中，释放一个锁，意味着内核中如果有线程正在等待该锁，则它的状态就可以变成运行态。比如，线程B的获取操作成功。但此时，内核只是将线程B标记为锁的所有者，而线程A继续执行。很快地，线程A又要获取锁了，由于该锁已经被标记给线程B了，所以线程A不得不放弃时间片，将控制权交给调度器。调度器终于可以捡起线程B，将处理器的执行权交给它。等到线程B释放了锁，下一个线程获得锁的所有权，并且等到线程B放弃执行权或者结束时间片之后就有机会被执行。此过程一直持续，经过一轮之后又会回到线程A，从而继续下一轮的争抢。在此期间，这些线程总是未执行满时间片就不得不放弃执行权。下面的图说明了三个线程在争抢一个锁时候的执行情况。

![img](http://img.blog.csdn.net/20150918163354780?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

假设一个线程在一个满时间片的执行过程中要多次获取/释放锁，它一旦释放了锁，则意味着，只要存在锁竞争，它在分配给它的当前时间片内已经无法再重新获得锁了。所以，它只能执行到它的下一次获取操作为止。譬如，参与竞争的线程平均执行1/3时间片就要获取锁，那么，线程的实际执行时间变成了1/3时间片。系统的调度粒度变成原来的1/3时间间隔。这引起了3倍数量的线程切换。从上图的右半部分可以看出，每个线程在一轮的循环中，只有1/3时间片的机会。这导致了3倍的线程切换。

除了引起调度粒度变小以外，lockconvoys的另一个问题是造成调度器的时间分配不公平。假设另有一个线程X也是在同等的优先级上运行，但没有参与锁竞争。于是，在每一轮的锁竞争过程中，线程X都有机会被分配一次完整的时间片，于是，这些竞争的线程在一轮中获得1/3时间片，而非竞争的线程可以获得完整的时间片。当然，你可以说这种不公平是由于它们抢锁而引起的，但从时间分配比例而言，参与竞争与不参与竞争的线程是不公平的。下图说明了线程X和A、B、C之间的执行时间差异。

![img](http://img.blog.csdn.net/20150918163627949?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

由以上描述可以看出，Lockconvoys的存在条件是，参与竞争的线程频繁地获取锁，锁被一个线程释放以后其所有权便落到了另一个线程的手里。在操作系统中，相同优先级的线程按照FIFO的顺序被调度和执行，竞争同一个锁的线程也按照FIFO的顺序被依次成功地获取到锁。这些条件在现代操作系统中都能被满足，包括Windows。

Lock convoys虽然不是致命的问题，但也可能在实际系统中发生。Sue Loh在她的博客文章中展示了在Windows CE中发生的lock convoy问题。她也讨论了一种合理的缓解lockconvoy的方案，要求在每个线程获取锁的时候先尝试（try），如果尝试多次仍不成功，再阻塞。



# 互斥同步锁（悲观锁）

* 悲观锁在操作数据时比较悲观，认为别人会同时修改数据。因此操作数据时直接把数据锁住，直到操作完成后才会释放锁；上锁期间其他人不能修改数据。
* 实现方式
	* 悲观锁的实现方式是加锁，加锁既可以是对代码块加锁（如Java的synchronized关键字），也可以是对数据加锁（如MySQL中的排它锁）

	哪些劣势：
		1. 阻塞，唤醒，性能低
		2. 永久阻塞
		3. 优先级，阻塞的优先级越高，持有锁的优先级越低，导致优先级反转问题
# 非互斥同步锁（乐观锁）

* 乐观锁在操作数据时非常乐观，认为别人不会同时修改数据。因此乐观锁不会上锁，只是在执行更新的时候判断* 实现方式
   * 乐观锁的实现方式主要有两种：CAS机制和版本号机制
      * CAS 3个操作数: 需要读写的内存位置(V) 进行比较的预期值(A) 拟写入的新值(B)
      * 如果内存位置V的值等于预期的A值，则将该位置更新为新值B，否则不进行任何操作。
      * CAS是由CPU支持的原子操作
      * Java中的自增操作(i++) AtomicInteger
   * CAS有哪些缺点
      * ABA问题 （线程1读取内存中数据为A；(2)线程2将该数据修改为B；(3)线程2将该数据修改为A；(4)线程1对数据进行CAS操作，在第(4)步中，由于内存中数据仍然为A，因此CAS操作成功，但实际上该数据已经被线程2修改过了。）带来栈顶问题，解决方法加入版本号
     * 在并发冲突概率大的高竞争环境下，如果CAS一直失败，会一直重试，CPU开销较大。针对这个问题的一个思路是引入退出机制，如重试次数超过一定阈值后失败退出。


      * CAS只能保证单个变量操作的原子性，当涉及到多个变量时
   *乐观锁加锁吗？
      * 乐观锁本身是不加锁的，只是在更新时判断一下数据是否被其他线程更新了；AtomicInteger便是一个例子
      * 有时乐观锁可能与加锁操作合作，MySQL在执行update时会加排它锁。但这只是乐观锁与加锁操作合作的例子，不能改变“乐观锁本身不加锁”这一事实。
4. 如何选择悲观锁和乐观锁
* 当竞争不激烈 (出现并发冲突的概率小)时，乐观锁更有优势，因为悲观锁会锁住代码块或数据，其他线程无法同时访问，影响并发，而且加锁和释放锁都需要消耗额外的资源。
* 当竞争激烈(出现并发冲突的概率大)时，悲观锁更有优势，因为乐观锁在执行更新时频繁失败，需要不断重试，浪费CPU资源。
5. 锁粗化
* 锁粗化就是将多个连续的加锁、解锁操作连接在一起，扩展成一个范围更大的锁。以此来减少在锁操作上的开销。
6. 自旋锁
* 所谓自旋锁，就是让某线程进入已被其它线程占用的同步代码时等待一段时间，不会被立即挂起，看持有锁的线程是否会很快释放锁。这里等待的方式就是执行一段无意义的循环。
7. 适应性自旋锁
* 适应性自旋锁是一种更加聪明的自旋锁。某个线程如果自旋成功了，那么下次自旋的次数会更加多，因为虚拟机认为既然上次成功了，那么此次自旋也很有可能会再次成功，那么它就会允许自旋等待持续的次数更多。反之，如果对于某个锁，很少有自旋能够成功的，那么在以后要或者这个锁的时候自旋的次数会减少甚至省略掉自旋过程，以免浪费处理器资源。



# 分布式锁

CAP理论告诉我们“任何一个分布式系统都无法同时满足**一致性**（Consistency）、**可用性**（Availability）和**分区容错性**（Partition tolerance），最多只能同时满足两项。在互联网领域的绝大多数的场景中，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证“最终一致性”，只要这个最终时间是在用户可以接受的范围内即可。

## 分布式锁应该具备哪些条件

1、在分布式系统环境下，一个方法在同一时间只能被一个机器的一个线程执行； 
2、高可用的获取锁与释放锁； 
3、高性能的获取锁与释放锁； 
4、具备可重入特性； 
5、具备锁失效机制，防止死锁； 
6、具备非阻塞锁特性，即没有获取到锁将直接返回获取锁失败。

## 分布式锁的三种实现方式

基于数据库实现分布式锁； 
基于缓存（Redis等）实现分布式锁； 
基于Zookeeper实现分布式锁；

## 分布式缓存集群

好了，有了前面的基础，再看分布式缓存集群就简单多了。我们只需要多考虑两点就够了。

### NO.1—— 取模

这是最简单但最不实用的一个。

以redis为例，假设我们有5台机器，要想取模肯定先得转换为数字，我们将一个请求的key转成数字（比如CRC16算法那），比如现在五个请求转换成的数字后对5取模分别为0、1、2、3、4，正好转发到五台机器上。

这时意外来了，其中一台宕机了，现在集群中还有4台。之后再来请求只能对4取模。问题就暴露出来了，那之前按照5取模的数据命中的机率大大降低了，相当于每宕机一台，之前存入的数据几乎都不能用了。

因此，不推荐此种做法。

### NO.2—— 哈希

这种算法叫哈希有些笼统了，具体可以分为ip哈希和url哈希（类似原地址散列）。这里就不多说了。重点说下redis中的设计。

redis中引入了哈希槽来解决这一问题。16384个哈希槽，每次不再对集群中服务器的总数取模，而是16384这个固定的数字。然后将请求分发。这样就可以避免其中一台服务器宕机，原有数据无法命中的问题。

### NO.3—— 一致性哈希

终于到重头戏了，不过有了前面的介绍，这个也就不难理解了。 
一致性哈希在memcached中有使用，通过一个hash环来实现key到缓存服务器的映射。

这个环的长度为2^32，根据节点名称的hash值将缓存服务器节点放在这个hash环上。如下图中的node1、node2等。

这时要保存的数据key仍旧进行hash运算，运算之后的值会落在这个hash环上的某一处（图中粉色的节点），当然这还没结束，因为他还没有落到node上。之后这个粉色的节点会沿顺时针落到离他最近的node上，over。

<img src=一致性Hash.png style="zoom:33%;" />

### 改进

当其中某个结点宕机后，比如node4，这时如果有结点落在了node2和node4之间，本来应该归为node4的，但这时候他宕机了，就都放在了下一个node3中。

这时，每个结点的数据量就会有很大出入了，平衡性很难保证。因此，引入“虚拟结点”。

> 虚拟结点是实际结点在hash空间的复制品，为了保证平衡性，一个实际node被划分成了若干个虚拟node。

如下图所示：

<img src=一致性hash改进.png style="zoom:33%;" />

服务器之间的交错关系会改变，每个虚拟node之间的相对关系是不一定的，比如每个node1的虚拟节点后面可以是node2的也可以跟node3的。

此时，如果红色的node宕机了，那么将要落在这台机器上的数据可能落到蓝色，也可能是绿色的。这就保证了一定程度的平衡性。